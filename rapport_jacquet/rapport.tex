\titre{Numerical simulations of LZ78 for Markovian sources}
% Veuillez ne pas modifier ce titre SVP.
% En cas de doute, prï¿½venez votre coordinateur.
% Compilez par: "latex main.tex".


\separation
\bk

% Quelques explications sur le sujet; articulation des parties; une page.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\medskip

\section{Conditions of the experiment}
\subsection{Simulation details}

Simulations in this report follow this experimental
process :
	
\begin{itemize}

	\item Generating a random Markov chain of size 2 of matrix
 \centers{ $\begin{matrice}
			p_{0 0} & p_{0 1} \\
			p_{1 0} & p_{1 1} \\
		  \end{matrice}$}	 
 \item
 Generating $n_{\text{exp}} \sim 10^3$ words of length $n $ (or $n_{\text{word}}$), with $n \sim 10^6 \text{ or } 10^7$
 
 \item Applying LZ78 on each of these words to estimate, for each $n$,
 the number of phrases $M_n$. A simple histogram of these values
 can be seen in figure 1.
 
 \item From this sampling of the random variable $M_n$ and other parameters such as the entropy of the Markov chain, computing
 
 	\begin{itemize}
 		\item the empirical mean ($\mu$) and the empirical variance ($\sigma^2$)
 		\item different theoretical expressions for the variance
 	\end{itemize}
 	
 \item Using these expressions to standardize $M_n$ in different ways, plotting
 
 	\begin{itemize}
 		\item the probability distribution of $M_n$ (standardized)
 			  
 		\item the cumulative distribution function of $M_n$ (standardized)
 	\end{itemize}
 
 \item Finally, comparing the different theoretical expressions for the variance 
 by plotting their differences for a large range of values of $n$, and
 a constant number of experiments $n_{\text{exp}}$.
\end{itemize}
 
%  \subsection{Example histogram}
%  This histogram represents the counts of the different 
%  values taken by $M_n$ for $n=10^6$. 
%  Each tick on the x-axis is a data point.
 
%  \centers{
%   \begin{minipage}{7cm}
   
    		
%         \includegraphics[width = 7cm,
%         				    trim = 27cm 0 0 0,
%         				    	clip=true]{M_n_raw_10e6_500.png}	
       
    
% 	\end{minipage}
% }

	\subsection{Empirical normalization}
 Using the empirical mean ($\mu$) and variance ($\sigma^2$) of the dataset to normalize $M_n$,
 this is a plot of the normalized distribution, compared to the normal distribution 
 in red :
 \centers{
  \begin{minipage}{6cm}
        \includegraphics[width = 6cm,
        				    trim = 26.7cm 0 0 30,
        				    	clip=true]{empirical_normalization_10e6_500.png}	
	\end{minipage} 
}
	\noindent
	 and its cumulative distribution function in green, compared to the normal one in red:
 	\centers{
 	 \begin{minipage}{7cm}
        \includegraphics[width = 7cm,
        				    trim = 27cm 0 0 32,
        				    	clip=true]{cdf_1e6_500.png}
	\end{minipage} 
	}
	
	These simulations and figures strongly indicate that the general distribution
	of $M_n$ respects the central limit theorem. We now experiment with
	candidates for the variance of $M_n$ : $V_n$

	% In comments, because not really interesting
	% \centers{\question{Theoretical mean}}
	% I also tried to normalize $M_n$ using theoretical expressions
	% of the mean and variance. For the mean, the first order expression
	
	% \centers{$E_n \sim \f{nh}{\log_2(n)}$}
	
	% \noindent
	% is, under $n\leq 10^6$, not sufficient to center the distribution. I conducted a numerical analysis
	% of the difference between this expression and the empirical mean for growing 
	% values of $n$. In particular, here is how their difference, in black, compares with
	% different approximation functions 
	
	% \centers{
 	%  \begin{minipage}{9cm}
    %     \includegraphics[width = 9cm,
    %     				    trim = 14cm 0 13cm 20,
    %     				    	clip=true]{mean_analysis_2e4_500.png}	
	% \end{minipage} 
	% }
	
	% \noindent
	% This is not troubling as it was already predicted in the formula:
	
	% \centers{$E_n = 	\f{nh}{\log_2(n)} + \mathcal{O} \pa{ \f{n}{\log_2(n)} }$}
	
	% \pagebreak
	\section{Validating variance candidates}
	\subsection{A first expression}
	As it will be used in the next section, this is the detail of the expression
	from \cite{nein} :

		\centers{ $V_n = \f{H^3 \sigma^2 n}{\log_2^2 (n)}$ }
		
	\centers{$\sigma^2 = \sigma_0^2 + \sigma_1^2$}
	\leftcenters{where}{$\sigma_i^2 = \f{\pi_i p_{i 0} p_{i 1}}{ H^3 } \pa { \log_2 \pa{ \f{ p_{i 0} }{ p_{i 1} } }
										+ \f{H_1 - H_0}{p_{0 1} + p_{1 0}} }^2$}
	\leftcenters{with}{$\pi_0 = \f{p_{1 0}}{p_{1 0} + p_{0 1}} \qquad \pi_1 = \f{p_{0 1}}{p_{1 0} + p_{0 1}}$}
	\leftcenters{and}{$H_i = -p_{i 0} \log_2(p_{i 0}) - p_{i 1} \log_2(p_{i 1}) \qquad H = \pi_0 H_0 + \pi_1 H_1 $}
	
	% \begin{remarque}
	% \noindent 
	% The first term in the squared part of $\sigma_i^2$ accounts for the expression of the variance for memoryless sources:
	
	% \begin{egalites}
	% & \ p_{i 0}\,p_{i 1} \log_2^2 \pa{ \f{p_{i 0} }{ p_{i 1} }}
	% 	& p_{i 0} \log_2^2(p_{i 0}) + p_{i 1} \log_2^2(p_{i 1}) 
	% 		- (- p_{i 0} \log_2(p_{i 0})  - p_{i 1} \log_2(p_{i 1}))^2 \\
	% 	&& h_2 - h^2
	% \end{egalites}
	
	% \end{remarque}	
	
	% \noindent
	% It seems, from simulations, that this variance is too small and doesn't
	% catch up with the empirical variance. Here is how they compare when plotted
	% together :
	
	% \centers{
 	%  \begin{minipage}{11cm}
    %     \includegraphics[width = 11cm,
    %     				    trim = 15 0 20cm 0,
    %     				    	clip=true]{std_analysis_2e4_500.png}	
	% \end{minipage} 
	% }
	
	% \pagebreak
	% \noindent
	% It seems, at first glance, that the increase would asymptotically be 
	% simply logarithmic
	
	% \centers{
 	%  \begin{minipage}{12cm}
    %     \includegraphics[width = 12cm,
    %     				    trim = 20.5cm 0 0 0,
    %     				    	clip=true]{std_analysis_approx_2e4_500.png}	
	% \end{minipage} 
	% }
	
	% \noindent 
	
	