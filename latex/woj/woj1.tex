\documentclass[11pt]{article}

%% duboc sty
\usepackage{amsmath}
\usepackage{amssymb}
\newtheorem{df}{Definition}
\newtheorem{rmk}{Remark}
\newtheorem{nota}{Notation}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}
\newcommand{\intxy}[4]{\left#3\,#1\, ;  \,#2\,\right#4}
\newcommand{\intoo}[2]{\intxy{#1}{#2}{]}{[}}
\newcommand{\limt}{\lim\limits}
\newcommand{\fonction}[5]{%
	#1\colon
	\left \{
		\begin{array}{@{}c@{\ }c@{\ }l}
			\medskip #2 & \longrightarrow & #3 \\
			#4 & \longmapsto & #5 \\
		\end{array}
	\right .
}
\newcommand{\union}[2]{\bigcup\limits_{#1}^{#2}}
\newcommand{\leftcenters}[3][2]{{\raggedright #2}{\centering#3}\raggedright}
\newcommand{\tf}[2]{#1/#2}
\newcommand{\proba}[1]{\text{P} \left( #1 \right) }
\newcommand{\question}[2][3]{%
	{\centering%
	\noindent\fbox{\bfseries#2}}\raggedright}%
\newcommand{\numero}[1]{\mathbf{(#1)}}
\newcommand{\centers}[2][2]{
    \begin{center}
    {#2}\end{center}}
\newcommand{\f}[2]{{
	\mathchoice%
		{\dfrac{#1}{#2}}
    	{\dfrac{#1}{#2}}
		{\frac{#1}{#2}}
		{\frac{#1}{#2}}
}}
\newcommand{\pa}[2][9]{\left( #2 \right)}
\newcommand\Sum[2]{\textstyle\sum\limits_{#1}^{#2}}
\newcommand{\SUM}[2]{{\displaystyle\sum\limits_{#1}^{#2}}}
\newcommand{\pinf}{\tiny{+}\infty}
\newcommand{\minf}{\tiny{-}\infty}
\newcommand{\pac}[2][9]{$\big[#2\big]$}
%% end duboc sty

\newtheorem{Proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\renewcommand{\baselinestretch}{1.1}
\newenvironment{comment}{\setbox0=\vbox\bgroup}{\egroup}
\newenvironment{singlespace}{\def\baselinestretch{1.0}\large\normalsize}{\par}

% Marginal notes and labeling macros
\newcommand\marginal[1]{\marginpar{\raggedright\parindent=0pt\tiny #1}}
\newcommand\WS{\marginal{WS}}

\setlength{\textheight}{8.7in}
\setlength{\textwidth}{6.1in}
\def\noi{\noindent}
\def\lf{\lfloor}
\def\rf{\rfloor}
\def\eod{\vrule height 6pt width 5pt depth 0pt}
\def\parsec{\par\noindent}
\def\big{\bigskip\parsec}
\def\med{\medskip\parsec}
\def\pr{{\bf P}}
\def\E{{\bf E}}
\def\l{\ell}
\def\eps{\varepsilon}
\def\lf{\lfloor}
\def\rf{\rfloor}
\def\Var{{\bf Var~}}
\def\W{{\cal W}}
\def\A{{\cal A}}
\def\B{{\cal B}}
\def\S{{\cal S}}
\def\lam{\lambda}
\def\T{{\cal T}}
\def\R{{\cal R}}
\def\U{{\cal U}}
\def\M{{\cal M}}
\def\TB{\widetilde{B}}


\vsize=8.7in
\voffset=-0.7in
\hoffset=-0.5in


\begin{document}

\begin{center}
{\LARGE {\bf Notes on Optimal Parsing pf LZ78 by Mignosi et al.}}
\big
G. Duboc ~ and ~ W. Szpankowski
\end{center}

\question{Corrections}

\begin{itemize}

\item The formal definition of ${D_n}^{\text{LZ}}$ in $\numero{3}$ is 
      misleading because it contradicts the previous verbal definition.
      For $w$ a word of size $n$, the formula
      \centers{$ \f{1}{M_n(w)} \Sum{j=0}{M_n(w)-1} |{u_j}^{\text{LZ}}|$}
      would rather be the empirical average length of a phrase in the Lempel-Ziv
      parsing of a word ${\overline D_n}^{\text{LZ}}$, whereas
      ${D_n}^{\text{LZ}}$ is used in the rest of the paper as the length
      of a randomly selected phrase. These two aren't equal :
      if we build a Lempel-Ziv DST from a word, then 
      ${D_n}^{\text{LZ}}$ can be seen as the depth of a random node, 
      which is different from the average path length computed 
      on all the nodes. 

\item In \emph{Remark 2}, I think the definition of $v$ and 
      $t$ should rather be 
      $ v = a_{i'} \dots a_{i} $ and
      $ t = a_{i+1} \dots a_n$. 

\item The result $\numero{14}$ should be an equality, and it is one indeed
      because of the flexible parsing algorithm. A proof by contradiction
      can show this. However, since we upperbound $g(j)$ by $\pinf$ in 
      $\numero{23}$ there might be no purpose to using $\numero{14}$ 
      instead of just $\numero{13}$. 
      % Furthermore, an equality is 
      % needed to establish result $\numero{22}$.

\item The proof around \emph{Theorem 1} has several flaws.
      The notation $X$ for a sequence depending on $n$ and not a 
      random variable is misleading. On the other hand, it should appear
      that both $g$ and $j$ are random variables, as the randomness of 
      $j$ is used in the end of the proof.
      I wrote some possible definitions \hyperlink{definitions}{here},
      and applied them to make some computations that seemed otherwise
      incorrect because of their use of randomness outside of a probability
      measure (\hyperlink{computations}{here}).

\item As for the arguments that 
      link $|L_{g(j)-k}|$ to ${D_n}^{\text{LZ}}$, I have
      indicated how I think they could be developped in 
      \hyperlink{critics}{this part.} These arguments
      are the most controversial part right now I think.

\item \emph{Theorem 2} is false as stated: we proved \emph{Theorem 1}
      using a random $j$. The randomness remains, so the quantifier
      'for any $j < M_n$' should be removed. This would be true for 
      \emph{Theorem 1} too.

\item The proof of \emph{Theorem 2} may stop at $\numero{26}$
      since we can directly prove this upperbound goes to $0$.
      This yields a tighter upperbound for \emph{Theorem 2}.
      I detailed this analysis in the 
      \hyperlink{upperbound}{last part of this report.}

\item In that same proof, the step between $\numero{25}$ and 
      $\numero{26}$ relies heavily on a result from \emph{[6]}.
      A bit more context on this result (and why it does apply here)
      would make things clearer.

\item The conclusion claims to use \emph{Cramer's} theorem to link
      \centers{$\underset{ 0 \leq k \leq g_{\scriptscriptstyle W}(J) }{ \max } 
                    \left\{ { L_{g_{\scriptscriptstyle W}(J)-k} - k } \right\}$}
      to ${D_n}^{\text{FLEX}}$, which is a sum of random variables.
      Since \emph{Cramer} applies to independent random variables and the 
      lengths of successive phrases are not independent, something must be missing 
      there. 
      
\end{itemize}\hypertarget{definitions}{\centers{\question{Notations}}}

\noindent
These are definitions and notations in order
to write the proof of \emph{Theorem 1}.

\begin{df}
For all $n\in\mathbb{N}$, calling $\Omega_n$ the set 
of words of length $n$.
\end{df}

\begin{df}
Defining $W \in \Omega_n$ to be a random variable which outputs 
words of length $n$ from a memoryless source.
\end{df}

\begin{df}
Considering $J \in \mathbb{N}^{\star}$ to be a random
variable which, in the event $\{ W = w \}$,
uniformly randomly picks the index of
one of the phrases of $w$.
The joint law of $J$ with $W$ being :

\centers{$ \proba{ W=w, J=j } = 
                    \begin{cases}
                        \tf{1}{M_n(w)} \quad & \text{ if } j \leq M_n(w) \\
                        0 \qquad \qquad  &\text{ else }
                    \end{cases} $}
\end{df}

\begin{rmk}
We might choose another randomness for $J$, but this one seems
more natural.
\end{rmk}

\begin{df}
For a given word $w\in\Omega_n$, 
and for all $i\in\mathbb{N}^{\star}$, 
we consider $g_w(i)$ defined by
\centers{$g_w(i) = f_w(i) + |L_{f_w(i)}|$ }
where $f_w(i)$ is the starting index
of the $i^{\text{th}}$ phrase of the flexible parsing of $w$,
and $|L_{f_w(i)}|$ is the length of the longest greedy phrase
given by the Lempel-Ziv parsing of this same word.
\end{df}

\begin{df}
We define $g_{\scriptscriptstyle W}(J)$ to be the random variable which
outputs $g_w(j)$ during the events $\{ W=w \}$ and $\{ J = j \}$.
\end{df}

\begin{df}
For all $k \in \mathbb{N}$, let $L_{g_{\scriptscriptstyle W}(J) - k}$ be the random
variables which gives the $(k+1)^{\text{th}}$ possible phrase 
for the flexible parsing at index $g_{\scriptscriptstyle W}(J) - k$. 
Its only randomness comes from $W$ and $J$. 
If $i\leq 0$, we might assume that $L_i$ will be the empty word of 
size 0.
\end{df}


\begin{nota}
    Denoting by
    \centers{$B_{J, W}^k = | L_{g_{\scriptscriptstyle W}(J) - k} |$}
    \noindent the length of this $(k+1)^{\text{th}}$ candidate.
\end{nota}


\noindent
We can now study the random variable
    \centers{$ \underset{ 0 \leq k \leq g_{\scriptscriptstyle W}(J) }{ \max } 
                    \left\{ { B_{J, W}^k - k } \right\} $}

\noindent
Given any $(j,w) \in \mathbb{N}^{\star} \times \Omega_n$,
under the events $\{ J = j \}$, $\{ W = w \}$ and $\{ J \leq M_n(W) \}$, 
this random variable is the length of 
the $j^{\textmd{th}}$ phrase of the flexible parsing of the word $w$.

\begin{df}
Defining the random variable ${D_n}^{\text{FLEX}}$, for $w\in\Omega_n$, as

\begin{align*}
    {D_n}^{\text{FLEX}} (w)
        &= \f{1}{M_n(w)} \Sum{j=0}{M_n(w)-1} |u_j^{FLEX}(w)| \\
        &= \f{1}{M_n(w)} \Sum{j=0}{M_n(w)-1}  
                                \underset{ 0 \leq k \leq g_w(j) }{ \max } 
                                \left\{ { B_{j, w}^k - k } \right\}
\end{align*}
\noindent
where ${D_n}^{\text{FLEX}}(w)$ is the empirical average of the lengths of the phrases
of the flexible parsing of a word $w$, contrary to
        $\underset{ 0 \leq k \leq g_w(J) }{ \max } 
        \left\{ { B_{J, w}^k - k } \right\} $,
with $J$ a random variable and $w$ fixed,
which is the length of a uniformly randomly selected 
phrase of the flexible parsing of $w$. 
\end{df}

\begin{df}
    We denote $x_n$ the average value of ${D_n}^{\text{LZ}}$ :
    \centers{$x_n = \f1{h} \log_2 \pa{ \f{nh}{\log_2(n)} }$}
\end{df}

\begin{rmk}
    The random variable ${D_n}^{\text{LZ}}$ is the
    length of a phrase randomly taken from the 
    Lempel-Ziv parsing 
    of a memoryless-generated word. It is not the same as
    the empirical average length of a phrase, which we can
    denote,
    for $w \in \Omega_n$ :
    \centers{$ \overline{D_n}^{\text{LZ}}(w)
                = \f{1}{M_n(w)}
                    \SUM{j=0}{M_n(w)-1} \left| 
                                        u_j^{\text{FLEX}}
                                        \right| $}
\end{rmk}

\begin{nota}
    We denote ${b_n}^{\delta}$ as
    \centers{${b_n}^{\delta} = x_n + (c_3 x_n)^{\delta}$}
\end{nota}

\noindent
We will study
    \centers
    {$ \proba{ 
        \underset{ 0 \leq k \leq g_{\scriptscriptstyle W}(J) }{ \max } 
        \left\{ { B_{J, W}^k - k } \right\}
            > {b_n}^{\delta} } $}
            
\pagebreak
\hypertarget{computations}{\centers{\question{Computations}}} 
With these definitions, 
we can do the formal computations at the beginning 
of the proof of \emph{Theorem 1}.
By conditionning on $W$ and $J$, we obtain

\begin{align*}
         \proba{ \underset{ 0 \leq k \leq g_W(J) }{ \max } 
        \left\{ { B_{J, W}^k - k } \right\} > {b_n}^{\delta} }
            &= \Sum{w \in \Omega_n}{} \Sum{j \in \mathbb{N}^{\star}}{} \,
                \proba{ \underset{ 0 \leq k \leq g_W(J) }{ \max } 
        \left\{ { B_{J, W}^k - k } \right\} > {b_n}^{\delta}, W=w, J=j } \\[5mm]
            &= \Sum{w \in \Omega_n}{} \Sum{j \in \mathbb{N}^{\star}}{} \,
                \proba{ \union{k=0}{g_w(j)} \left\{ B_{w, j}^k > k + {b_n}^{\delta} \right\} 
                            ,W=w, J=j }\\[5mm]
            &\leq
                \Sum{w \in \Omega_n}{} \Sum{j \in \mathbb{N}^{\star}}{}
                \Sum{k=0}{g_w(j)} 
                \proba{ B_{w, j}^k > k + {b_n}^{\delta}, W=w, J=j } \\[5mm]
            &\leq
                \Sum{w \in \Omega_n}{} \Sum{j \in \mathbb{N}^{\star}}{}
                \Sum{k=0}{\pinf} \,
                \proba{ B_{w, j}^k > k + {b_n}^{\delta}, W=w, J=j }
                 \\[5mm]
            &=
                \Sum{k=0}{\pinf}
                \Sum{w \in \Omega_n}{} \Sum{j \in \mathbb{N}^{\star}}{} \,
                \proba{ B_{w, j}^k > k + {b_n}^{\delta}, W=w, J=j }
                 \\[5mm]
            &=
                \Sum{k=0}{\pinf} \,
                \proba{ B_{W, J}^k > k + {b_n}^{\delta} } \\
\end{align*}




\noindent
For all $k\in\mathbb{N}$, we may now prove that
    \centers{
        $ \proba{ B_{J, W}^k > k + {b_n}^{\delta} } 
            \leq \proba{ {D_n}^{\text{LZ}} > k + {b_n}^{\delta} } $
    }


\noindent
\hypertarget{critics}{}
Let $k \in\mathbb{N}$.
Currently, the proof to show this stands on three arguments :

\begin{itemize}
    \item[(1)] The first is that $L_{g_W(J)-k}$ is  
          a random phrase from the Lempel-Ziv parsing of 
          a word of length $N$, where $N \leq g_W(J) \leq n$.
          In the event $\{ N = n' \}$, we consider 
          ${D_{n'}}^{\text{LZ}}$.

    \item[(2)] The second, is that $|L_{g_W(J)-k}|$ can 
          therefore be considered the same as ${D_N}^{\text{LZ}}$
          \textit{i.e} at least equal in law.

    \item[(3)] The third is, roughly, that for all $n'\leq n$, 
        ${D_n'}^{\text{LZ}} \leq {D_n}^{\text{LZ}}$.
\end{itemize}
Although they seem generally true,
there are different problems with each of these arguments :

\begin{itemize}
    \item $N$ isn't clearly established, so 
          ${D_N}^{\text{LZ}}$ isn't really known. 

    \item If we can identify $N$ and, let's say,
          condition our probability with $\{ N = n' \}$, 
          it is not obvious
          that the choice of a phrase at position ${g_W(J)-k}$
          knowing $\{ N = n' \}$
          is the same as the uniform choice that operates
          when choosing a random phrase from a word of size
          $n'$, in ${D_{n'}}^{\text{LZ}}$.

    \item As for (3), this result is true on average, but 
          not in all cases. Indeed, since ${D_n}^{\text{LZ}}$
          (resp. ${D_n'}^{\text{LZ}}$) is concentrated
          around $x_n$ (resp. $x_n'$), and $x_n' < x_n$
          since $n' < n$, we can show 
          that this result holds with high probability.
          To write the proof, we may condition using
          events of the type

          \centers{$\{ | {D_n}^{\text{LZ}} - x_n | \leq {k_n}^{(1)} v_n \}$}

          \leftcenters
            {and}
            {$\{ | {D_{n'}}^{\text{LZ}} - x_{n'} | \leq {k_n}^{(2)} v_{n'} \} $}
          where 
          \centers{$ v_n = \sqrt{ \log( \tf{nh}{\log(n)} ) } $}
          \noindent
          A sketch of the proof is to
          apply concentration 
          inequalities to these events 
          while picking $({k_n}^{(1)}, {k_n}^{(2)})$ such that 
            \centers{$ {k_n}^{(1)} \, v_n + {k_n}^{(2)} \, v_{n'} < x_n - x_{n'} $}
          \noindent
          and having ${k_n}^{(1)}$ and ${k_n}^{(2)}$ go to $\pinf$ for 
          $n$ going to $\pinf$ in order for the upperbound probability
          to converge to zero. 
\end{itemize}


\hypertarget{upperbound}{\centers{\question{Upperbound proof}}}
 This is a proof that the upperbound 
in $\numero{26}$ goes to 0. Assuming

\begin{equation} \tag{$\numero{26}$}
     \SUM{k=0}{\pinf} 
            \proba{ D_n^{LZ} (W) > k + b_n^{\,\delta} }
            \leq 
            A \alpha^{ {(c_3 x_n)}^{\delta - \tf12} } 
                \Sum{i=0}{\pinf} \alpha^{ \tf{ i }{ \sqrt{ c_3 x_n } } }
\end{equation}

\noindent We can prove directly that the upperbound term goes to 0 
for $n$ going to $\pinf$,
without resorting to another majoration.
Since $\sqrt{c_3 x_n}$ goes to infinity for $n \rightarrow \pinf$,
we can pick $n$ such that $\sqrt{c_3 x_n} > 1$. Therefore
$\alpha^{\tf{1}{\sqrt{c_3 x_n}}} < 1$ and the geometric sum gives

\centers{ $ \SUM {i=0}{\pinf}
            \alpha^{ \tf{i}{\sqrt{c_3 x_n}} } =
                  \f { 1 } 
                     { 1 - \alpha^{ \tf 1 { \sqrt{ c_3 x_n } } } } $}

\noindent It remains to prove that 
    \centers{$ \underset{n\rightarrow\pinf}{\limt} \,\,\,
                  \f { \alpha^{ {(c_3 x_n)}^{ \delta - \tf12} } } 
                     { 1 - \alpha^{ \tf 1 { \sqrt { c_3 x_n } } }} = 0
            $}

which is done by using L'Hospital's rule.
Define :
\centers
    {$ \fonction
            {f}
            { {\R_{\tiny{+}}}^* }
            { \R }
            { x }
            { \alpha^{ x^{ \delta - \tf12 } } }
      \qquad \text{ and } \qquad
      \fonction
        {g}
        { {\R_{\tiny{+}}}^* }
        { \R }
        { x }
        {  1 - \alpha^{ \f1{ \sqrt{x} } } } $}

\noindent Let $x \in \intoo{0}{\pinf}$. Derivating yields :

\centers{$ f'(x) = \ln \alpha \pa {\delta - \f12}
                    \, x^{\delta - \tf32}
                    \, f(x)
            \qquad \text{ and } \qquad
     g'(x) 
        =  \ln \alpha\,\, \f 1 {2x\sqrt{x}}\, \alpha^{\f 1 {\sqrt{x}}} $}

We proceed to show that $\f{f'(x)}{g'(x)}$ goes to 0 as $x$ goes
to $\pinf$. We have

   \centers {
        $ \f{f'(x)}{g'(x)}
             = \f { \pa{\delta - \f12}
                    \, x^{\delta - \tf32}
                    \alpha^{ x^{ \delta - \tf12 } } }
                  { \f{1}{2x\sqrt{x}} 
                        \alpha^{ \f{1}{\sqrt{x}} } }
        = 2 \pa{\delta -\f12} x^{\delta} 
            \cdot \f { \alpha^{x^{\delta-\tf12}}}
                      { \alpha^{ \f{1}{\sqrt{x}} } } $} 

Since $\alpha^{\f{1}{\sqrt{x}}} 
        \underset{x\rightarrow\pinf}{\longrightarrow} 1$, we are left to study 
        $ x^{\delta} \alpha^{x^{\delta-\tf12}}$. Writing

 

\centers{$x^{\delta} \alpha^{x^{\delta-\tf12}} 
            = e^{ \delta \log x + \log \alpha \cdot x^{\delta - \tf12} }$}

and taking the log, since $\delta > \tf12$ and $\log \alpha < 0$ we see that

\centers{$ \delta \log x + \log \alpha \cdot x^{\delta - \tf12} 
            \underset{x\rightarrow\pinf}{\longrightarrow} \minf$}

Therefore $x^{\delta} \alpha^{x^{\delta-\tf12}} \underset{x\rightarrow\pinf}{\rightarrow} 0$,
and given that $f(0) = g(0) = 0$, L'Hospital's rule applies, proving that 
$\f{f(x)}{g(x)} \underset{x\rightarrow\pinf}{\longrightarrow} 0$.


\end{document}



            


