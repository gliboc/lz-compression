"""Expressions from the paper Szpan. & Jacquet

Execute this script to see somes values 
printed in the terminal and in the file table.dat"""

from math import log
import numpy as np
from markov import stationary_distribution, markov_chain


def entropy(M, p=None):
    """Computes the entropy of a known Markov chain by computing a
    stationary distribution.

    Args:
     M (float matrix): The Markov chain.
     [p] (float array): A previously computed stationary distribution.

    Returns:
      (float): The entropy of M.

    """

    if p is None:
        p = stationary_distribution(M)

    h = 0
    n = len(M)

    for i in range(n):
        for j in range(n):

            h += p[i] * M[i, j] * log(M[i, j])

    return -h  # verified


def h_2(M, p=None, h=None):
    r"""Computes the second derivative of lambda, taken in s=-1
    (see Average profile of the Lempel-Ziv parsing scheme for a Markovian source).

    Args:
      M (float matrix): The Markov chain.
      p (float array): A left eigenvector.
      [h] (float): The precomputed entropy of M.

    Returns:
      (float): \dot{\dot {lambda }}(-1)
    """

    if p is None:
        p = stationary_distribution(M)

    if h is None:
        h = entropy(M, p=p)

    n = len(M)

    t1 = 0
    t2 = 0
    t3 = 0

    for i in range(n):
        for j in range(n):

            t1 += p[i] * M[i, j] * log(M[i, j], 2) ** 2

    for i in range(n):
        for j in range(n):

            t2 += log(p[i], 2) * log(M[i, j], 2) * p[i] * M[i, j]

    t2 *= 2

    for i in range(n):
        for j in range(n):

            t3 += p[i] * log(p[i], 2)

    t3 *= 2 * h

    # print("h2's terms are %f, %f, %f" % (t1, t2, t3))
    return t1 + t2 + t3


def test_h2():
    """Testing the h2 function."""

    print("On first order Markov chains:")

    for _ in range(10):

        M = markov_chain(2)
        print(M)
        print("Has entropy %f" % entropy(M))
        print("Has stationary distribution:")
        p = stationary_distribution(M)
        print(p)
        print("The stationary distribution entropy is:")
        print(sum([-x * log(x, 2) for x in p]))
        print("Its h2 is:")
        input(h_2(M))


def H(M):
    """Entropy using base 2 logarithm"""

    p = stationary_distribution(M)
    h = 0
    n = len(M)

    for i in range(n):
        for j in range(n):

            h += p[i] * M[i, j] * log(M[i, j], 2)

    return -h  # verified


def omega(M):
    return (M[0, 1] + M[1, 0]) # verified


# def beta(M):
#     s = 0

#     p00 = M[0, 0]
#     p11 = M[1, 1]
#     p01 = M[0, 1]
#     p10 = M[1, 0]

#     s += p00 * p11 * (log(p00)) ** 2 * (log(p11)) ** 2
#     s -= p01 * p10 * (log(p01)) ** 2 * (log(p10)) ** 2

#     return s  # verified

def beta(M):
    p00 = M[0, 0]
    p01 = M[0, 1]
    p11 = M[1, 1]
    p10 = M[1, 0]

    beta = 0
    beta -= log(p00) ** 2 * p00 * (1 - p11)
    beta += log(p00) * log(p11) * p00 * p11
    beta -= log(p11) ** 2 * p11 * (1 - p00)
    beta += log(p11) * log(p00) * p11 * p00
    beta -= log(p01*p10) ** 2 * p01 * p10

    return beta

def beta2(M):
    s = 0

    p00 = M[0, 0]
    p11 = M[1, 1]
    p01 = M[0, 1]
    p10 = M[1, 0]

    s += 2 * log(p00) * log(p11) * (p00 * p11)
    s += 2 * log(p10) * log(p10) * (p10 * p01)
    s -= log(p00) ** 2 * p00 * p10
    s -= log(p11) ** 2 * p11 * p01
    s -= log(p10) ** 2 * p10 * p00
    s -= log(p01) ** 2 * p01 * p11

    return s  # verified


def pi_q_psi(M, p):
    r""" \pi \dot{Q}^{\star} \psi computation"""
    s = 0

    p00 = M[0, 0]
    p11 = M[1, 1]
    p01 = M[0, 1]
    p10 = M[1, 0]

    s += p[0] * p11 * log(p11)
    s -= p[1] * p10 * log(p10)
    s -= p[0] * p01 * log(p01)
    s += p[1] * p00 * log(p00)

    return s  # verified


def variances(M, n):
    r"""Computes Var[D_m] and its parts from 'Average profile of the LZ parsing for
    Markovian sources.
    Note that the result is given modulo a O(1)"""

    b = beta(M)
    o = omega(M)
    p = [M[1, 0] / o, M[0, 1] / o]
    h = 0.
    h -= p[0] * M[0, 0] * log(M[0, 0])
    h -= p[0] * M[0, 1] * log(M[0, 1])
    h -= p[1] * M[1, 0] * log(M[1, 0])
    h -= p[1] * M[1, 1] * log(M[1, 1])

    # approximation for m (m sequences generated by m different markov sources,
    # but we are in the case n fixed here)
    m = n * h / log(n)

    s = 0.

    s -= b / o
    bo = b / o
    print("b/o =", b / o)

    s -= 2 * pi_q_psi(M, p) / o
    pqp = 2 * pi_q_psi(M, p) / o
    print("2 / o * pqpsi =", 2 * pi_q_psi(M, p) / o)

    s -= h ** 2
    h2 = h ** 2
    print("h^2 =", h ** 2)

    print("The inner part of Var[D_m] is worth ", s)

    s /= h ** 3

    s *= log(m)

    return (s, (o, h ** 3, 1 / h ** 3, bo, pqp, h2, log(m)))  # verified


def var(M, n):
    r"""Computes Var[D_m] from 'Average profile of the LZ parsing for
    Markovian sources.
    Note that the result is given modulo a O(1)"""

    b = beta(M)
    p = stationary_distribution(M)
    o = omega(M)
    h = entropy(M)

    # approximation for m (m sequences generated by m different markov sources,
    # but we are in the case n fixed here)
    m = n * h / log(n)

    s = 0.

    s -= b / o
    bo = b / o
    print("b/o =", b / o)

    s -= 2 * pi_q_psi(M, p) / o
    print("2 / o * pqpsi =", 2 * pi_q_psi(M, p) / o)

    s -= h ** 2
    print("h^2 =", h ** 2)

    print("The inner part of Var[D_m] is worth ", s)

    s /= h ** 3

    s *= log(m)

    return s  # verified


def psi(n):
    """Returns the Psi vector: a 1-D array of size n filled with ones.

    Args:
        n (int): Size of the Markov chain.

    Returns:
        (int array): The Psi vector
    """

    return np.ones(n)


if __name__ == "__main__":
    import pandas as pd

    os, hs, unhs, vns, h2s, pqps, bos, lns = [], [], [], [], [], [], [], []

    for _ in range(10):
        m_chain = markov_chain(2)
        n = 500

        (v, (o, h3, unh, bo, pqp, h2, lg)) = variances(m_chain, n)
        print("For M =", m_chain)
        print("n =", n)
        print("var =", var(m_chain, n))

        os.append(o)
        hs.append(h3)
        unhs.append(unh)
        vns.append(v)
        h2s.append(h2)
        bos.append(bo)
        pqps.append(pqp)
        lns.append(lg)

    d = {
        "omegas": os,
        "h^3 values": hs,
        "unhs": unhs,
        "beta/omega": bos,
        "2 / omega pi_q_psi": pqps,
        "h^2": h2s,
        "logs": lns,
        "V_n": vns,
    }

    df = pd.DataFrame(data=d)

    print("Writing a table")
    with open("table.dat", "w") as fo:
        x = df.to_latex()
        for l in x:
            fo.write(l)

    print("Some values of omega", os)
    print("Some values of h^3", hs)
    print("Some values of V_n", vns)

    # test_h2()